{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"fast-rlm","text":"<p>A minimal, pure Python implementation of Recursive Language Models (RLMs) for handling arbitrarily long context.</p> <p> </p> <p>GitHub | PyPI | Paper</p> <p>Pure Python (v0.2.0+)</p> <p>fast-rlm now uses a pure Python implementation. No Deno or external runtimes required!</p>"},{"location":"#what-are-rlms","title":"What are RLMs?","text":"<p>RLMs are an inference technique where an LLM interacts with arbitrarily long prompts through an external REPL. The LLM can:</p> <ul> <li>Write code to explore, decompose, and transform the prompt</li> <li>Recursively invoke sub-agents to complete smaller subtasks</li> <li>Work with contexts far beyond any model's window (millions of tokens)</li> </ul> <p>Crucially, sub-agent responses are not automatically loaded into the parent agent's context \u2014 they are returned as symbols or variables inside the parent's REPL.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>pip install fast-rlm\nexport RLM_MODEL_API_KEY=sk-or-...\n</code></pre> <pre><code>import fast_rlm\n\nresult = fast_rlm.run(\"Generate 50 fruits and count number of r\")\nprint(result[\"results\"])\n</code></pre>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>\u2705 Pure Python - No external runtimes (Deno, Node, etc.)</li> <li>\u2705 Arbitrarily long context - Process millions of tokens</li> <li>\u2705 Recursive subagents - Decompose complex tasks</li> <li>\u2705 Budget controls - Token and cost limits</li> <li>\u2705 JSONL logging - Detailed execution traces</li> <li>\u2705 TUI log viewer - Interactive debugging</li> </ul>"},{"location":"#example-long-context","title":"Example: Long Context","text":"<pre><code>import fast_rlm\n\ntranscripts = open(\"lex_fridman_all_transcripts.txt\").read()  # millions of tokens\n\nresult = fast_rlm.run(\n    \"Summarize what the first 5 ML guests said about AGI.\\n\\n\" + transcripts\n)\nprint(result[\"results\"])\n</code></pre> <p>The agent will automatically: - Search and filter transcripts programmatically - Chunk data intelligently - Spawn sub-agents for parallel processing - Aggregate results</p> <p>No manual RAG pipeline needed!</p>"},{"location":"#architecture","title":"Architecture","text":"<pre><code>User Query (arbitrarily long)\n      \u2193\nPrimary Agent (Python REPL)\n      \u251c\u2500\u2192 Writes code to explore context\n      \u251c\u2500\u2192 Spawns sub-agents recursively\n      \u2502   \u251c\u2500\u2192 Sub-agent 1 (processes chunk 1)\n      \u2502   \u251c\u2500\u2192 Sub-agent 2 (processes chunk 2)\n      \u2502   \u2514\u2500\u2192 Sub-agent N (processes chunk N)\n      \u2514\u2500\u2192 Aggregates results \u2192 Final Answer\n</code></pre>"},{"location":"#installation","title":"Installation","text":"<p>See Installation Guide</p>"},{"location":"#configuration","title":"Configuration","text":"<pre><code>from fast_rlm import run, RLMConfig\n\nconfig = RLMConfig.default()\nconfig.primary_agent = \"anthropic/claude-sonnet-4\"\nconfig.max_depth = 5\nconfig.max_money_spent = 2.0\n\nresult = run(\"Your query\", config=config)\n</code></pre> <p>See Configuration for all options.</p>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Getting Started</li> <li>Configuration</li> <li>Best Practices</li> <li>Development</li> </ul>"},{"location":"#implementation","title":"Implementation","text":"<p>fast-rlm is implemented in pure Python:</p> <pre><code>fast_rlm_py/\n\u251c\u2500\u2500 engine.py          # Core RLM loop\n\u251c\u2500\u2500 llm_client.py      # OpenAI API client\n\u251c\u2500\u2500 prompts.py         # System prompts\n\u251c\u2500\u2500 logger.py          # JSONL logging\n\u251c\u2500\u2500 ui.py              # Terminal UI (Rich)\n\u2514\u2500\u2500 usage_tracker.py   # Token tracking\n</code></pre> <ul> <li>Native Python execution (faster than WASM)</li> <li>Easy to extend (add custom tools)</li> <li>Simple deployment (just <code>pip install</code>)</li> <li>Standard debugging (native Python traces)</li> </ul>"},{"location":"#paper","title":"Paper","text":"<p>Based on the research paper: arxiv.org/abs/2512.24601</p>"},{"location":"#support","title":"Support","text":"<p>If you find this helpful, consider supporting on Patreon.</p>"},{"location":"#license","title":"License","text":"<p>MIT</p>"},{"location":"development/benchmarks/","title":"Benchmarks","text":"<p>fast-rlm includes evaluation scripts under <code>benchmarks/</code> for testing against standard long-context datasets.</p>"},{"location":"development/benchmarks/#setup","title":"Setup","text":"<p>Install benchmark dependencies:</p> <pre><code>uv sync --extra benchmarks\n</code></pre> <p>This adds the <code>datasets</code> library from Hugging Face.</p>"},{"location":"development/benchmarks/#available-benchmarks","title":"Available Benchmarks","text":""},{"location":"development/benchmarks/#longbench-narrativeqa","title":"LongBench (NarrativeQA)","text":"<p>Dataset: THUDM/LongBench \u2014 a multi-task benchmark for long context understanding.</p> <p>What it tests: Reading comprehension over long narratives. The agent receives a full story plus a question, and must find the answer by exploring the text through its REPL.</p> <pre><code>uv run benchmarks/longbench_benchmark.py\n</code></pre> Full source: <code>benchmarks/longbench_benchmark.py</code> <pre><code>import fast_rlm\nfrom datasets import load_dataset\n\nds = load_dataset(\"THUDM/LongBench\",\n                  \"narrativeqa\",\n                  split=\"test\",\n                  trust_remote_code=True)\nidx = 140\n\nexample = ds[idx]\n\nquery = f\"\"\"\n{example['input']}\n\n{example['context']}\n\"\"\"\n\ndata = fast_rlm.run(query, prefix=f\"longbench_hotpot_idx{idx}\")\nprint(\"Expected answer: \", example['answers'])\n</code></pre> <p>To test a different example, change <code>idx</code>:</p> <pre><code>idx = 100  # try different indices\n</code></pre>"},{"location":"development/benchmarks/#oolong-synth","title":"Oolong Synth","text":"<p>Dataset: oolongbench/oolong-synth \u2014 synthetic long-context tasks including timeline ordering, user tracking, and counting.</p> <p>What it tests: Precise information extraction from very long synthetic contexts. Tasks include tracking timelines, counting occurrences, and following user actions across large documents.</p> <pre><code>uv run benchmarks/oolong_synth_benchmark.py\n</code></pre> Full source: <code>benchmarks/oolong_synth_benchmark.py</code> <pre><code>import fast_rlm\nfrom datasets import load_dataset\n\nds = load_dataset(\"oolongbench/oolong-synth\",\n                  split=\"test\")\nidx = 100\n\nexample = ds[idx]\nprint(example['answer'])\n\nquery = f\"\"\"\n{example['context_window_text_with_labels']}\n\n{example['question']}\n\"\"\"\n\ndata = fast_rlm.run(query, prefix=f\"oolong_synth_idx{idx}\")\nprint(\"Expected answer: \", example['answer'])\n</code></pre> <p>You can filter by task type:</p> <pre><code># Available task groups: 'timeline', 'user', 'counting'\nds = ds.filter(lambda x: x['task_group'] == 'counting')\n</code></pre>"},{"location":"development/benchmarks/#adding-new-benchmarks","title":"Adding New Benchmarks","text":"<p>Create a new file in <code>benchmarks/</code>. The pattern is simple:</p> <pre><code>import fast_rlm\nfrom datasets import load_dataset\n\n# 1. Load a dataset\nds = load_dataset(\"your/dataset\", split=\"test\")\n\n# 2. Pick an example\nexample = ds[0]\n\n# 3. Build a query (question + context)\nquery = f\"{example['question']}\\n\\n{example['context']}\"\n\n# 4. Run it\nresult = fast_rlm.run(query, prefix=\"my_benchmark\")\n\n# 5. Compare\nprint(\"Got:\", result[\"results\"])\nprint(\"Expected:\", example[\"answer\"])\nprint(\"Cost:\", result[\"usage\"][\"cost\"])\n</code></pre> <p>The <code>usage</code> field in every result gives you per-run cost and token tracking, useful for comparing efficiency across models and configurations.</p>"},{"location":"development/from-source/","title":"Development from Source","text":""},{"location":"development/from-source/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+</li> <li>uv (recommended) or pip</li> <li>Bun (optional - only for log viewer)</li> </ul> <p>No Deno Required</p> <p>As of version 0.2.0, fast-rlm uses a pure Python implementation. Deno is no longer required!</p>"},{"location":"development/from-source/#setup","title":"Setup","text":"<pre><code>git clone https://github.com/avbiswas/fast-rlm.git\ncd fast-rlm\n</code></pre>"},{"location":"development/from-source/#install-python-dependencies","title":"Install Python dependencies","text":"uv (recommended)pip <pre><code>uv sync\n</code></pre> <pre><code>pip install -e .\n</code></pre>"},{"location":"development/from-source/#optional-install-log-viewer-dependencies","title":"(Optional) Install log viewer dependencies","text":"<pre><code>cd tui_log_viewer &amp;&amp; bun install &amp;&amp; cd ..\n</code></pre>"},{"location":"development/from-source/#set-your-api-key","title":"Set your API key","text":"<p>Create a <code>.env</code> file in the project root:</p> <pre><code>RLM_MODEL_API_KEY=sk-or-...\nRLM_MODEL_BASE_URL=https://openrouter.ai/api/v1\n</code></pre> <p>Or use <code>.envrc</code> with direnv:</p> <pre><code>export RLM_MODEL_API_KEY=sk-or-...\nexport RLM_MODEL_BASE_URL=https://openrouter.ai/api/v1  # optional, this is the default\n</code></pre> Variable Description Default <code>RLM_MODEL_API_KEY</code> API key for your LLM provider (required) <code>RLM_MODEL_BASE_URL</code> OpenAI-compatible base URL <code>https://openrouter.ai/api/v1</code>"},{"location":"development/from-source/#configuration","title":"Configuration","text":"<p>Edit <code>rlm_config.yaml</code> at the project root:</p> <pre><code>max_calls_per_subagent: 20\nmax_depth: 3\ntruncate_len: 2000\nprimary_agent: \"z-ai/glm-5\"\nsub_agent: \"minimax/minimax-m2.5\"\nmax_money_spent: 1.0\nmax_completion_tokens: 50000\nmax_prompt_tokens: 200000\n</code></pre>"},{"location":"development/from-source/#running","title":"Running","text":"<pre><code># Run the counting-r example\npython test_counting_r.py\n\n# Or with uv\nuv run python test_counting_r.py\n\n# Run the pure Python test\npython test_python_rlm.py\n\n# View logs\n./viewlog logs/&lt;logfile&gt;.jsonl\n</code></pre>"},{"location":"development/from-source/#editable-python-install","title":"Editable Python install","text":"<p>To develop the package locally:</p> <pre><code># With uv (recommended)\nuv sync\n\n# With pip\npip install -e .\n</code></pre> <p>Changes to <code>fast_rlm/</code> and <code>fast_rlm_py/</code> are reflected immediately \u2014 no rebuild needed.</p>"},{"location":"development/from-source/#project-structure","title":"Project structure","text":"<pre><code>fast-rlm/\n\u251c\u2500\u2500 fast_rlm/              # Python package (original API)\n\u2502   \u251c\u2500\u2500 __init__.py        # Public API: run(), RLMConfig\n\u2502   \u251c\u2500\u2500 _runner.py         # Uses fast_rlm_py backend\n\u2502   \u2514\u2500\u2500 _cli.py            # fast-rlm-log CLI entry point\n\u251c\u2500\u2500 fast_rlm_py/           # Pure Python implementation\n\u2502   \u251c\u2500\u2500 engine.py          # Core recursive agent loop\n\u2502   \u251c\u2500\u2500 llm_client.py      # LLM API client (OpenAI)\n\u2502   \u251c\u2500\u2500 prompts.py         # System prompts\n\u2502   \u251c\u2500\u2500 logger.py          # JSONL logger\n\u2502   \u251c\u2500\u2500 ui.py              # Terminal UI (Rich)\n\u2502   \u2514\u2500\u2500 usage_tracker.py   # Token/cost tracking\n\u251c\u2500\u2500 tui_log_viewer/        # OpenTUI log viewer (Bun)\n\u251c\u2500\u2500 benchmarks/            # Evaluation scripts\n\u251c\u2500\u2500 examples/              # Usage examples\n\u251c\u2500\u2500 rlm_config.yaml        # Default agent configuration\n\u2514\u2500\u2500 pyproject.toml         # Python build config (hatchling)\n</code></pre>"},{"location":"development/from-source/#running-benchmarks","title":"Running Benchmarks","text":"<pre><code># Install benchmark dependencies\nuv sync --extra benchmarks\n\n# Run LongBench\nuv run python benchmarks/longbench_benchmark.py\n\n# Run Oolong\nuv run python benchmarks/oolong_synth_benchmark.py\n</code></pre>"},{"location":"development/from-source/#testing","title":"Testing","text":"<pre><code># Simple test\npython test_counting_r.py\n\n# Pure Python API test\npython test_python_rlm.py\n\n# With environment loaded\nexport $(cat .env | grep -v '^#' | xargs)\nuv run python test_python_rlm.py\n</code></pre>"},{"location":"development/from-source/#adding-custom-tools","title":"Adding Custom Tools","text":"<p>It's easy to extend the Python implementation with custom tools:</p> <pre><code># Edit fast_rlm_py/engine.py, around line 99\n\n# Add your custom tool\nfrom elasticsearch import Elasticsearch\nes = Elasticsearch(['http://localhost:9200'])\n\ndef es_search(index: str, query: dict):\n    return es.search(index=index, body=query)\n\n# Inject into REPL environment\nrepl_globals['es_search'] = es_search\nrepl_locals['es_search'] = es_search\n\n# Now agents can use: results = es_search(\"docs\", {...})\n</code></pre>"},{"location":"development/from-source/#migration-from-typescript","title":"Migration from TypeScript","text":"<p>The TypeScript/Deno implementation has been migrated to pure Python:</p> Old (TypeScript) New (Python) <code>src/subagents.ts</code> <code>fast_rlm_py/engine.py</code> <code>src/call_llm.ts</code> <code>fast_rlm_py/llm_client.py</code> <code>src/prompt.ts</code> <code>fast_rlm_py/prompts.py</code> <code>src/logging.ts</code> <code>fast_rlm_py/logger.py</code> <code>src/ui.ts</code> <code>fast_rlm_py/ui.py</code> <code>src/usage.ts</code> <code>fast_rlm_py/usage_tracker.py</code> <p>See <code>FILE_MIGRATION_MAP.md</code> for detailed migration guide.</p>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#1-install-fast-rlm","title":"1. Install fast-rlm","text":"<pre><code>pip install fast-rlm\n</code></pre> <p>That's it! No additional runtime dependencies needed.</p> <p>Pure Python</p> <p>As of version 0.2.0, fast-rlm uses a pure Python implementation. No Deno installation required!</p>"},{"location":"getting-started/installation/#2-set-your-api-key","title":"2. Set your API key","text":"<p>fast-rlm uses OpenRouter by default. Set your API key:</p> <pre><code>export RLM_MODEL_API_KEY=sk-or-...\n</code></pre> <p>Tip</p> <p>Add this to your <code>.bashrc</code>, <code>.zshrc</code>, or <code>.envrc</code> so it persists across sessions.</p>"},{"location":"getting-started/installation/#3-optional-install-bun","title":"3. (Optional) Install Bun","text":"<p>Only needed if you want the interactive TUI log viewer (<code>fast-rlm-log &lt;file&gt; --tui</code>).</p> macOS / LinuxWindows <pre><code>curl -fsSL https://bun.sh/install | bash\n</code></pre> <p>Then add Bun to your <code>PATH</code> (the installer will print the exact lines, but typically):</p> <pre><code>export BUN_INSTALL=\"$HOME/.bun\"\nexport PATH=\"$BUN_INSTALL/bin:$PATH\"\n</code></pre> <pre><code>powershell -c \"irm bun.sh/install.ps1 | iex\"\n</code></pre> <p>Or install via npm:</p> <pre><code>npm install -g bun\n</code></pre> <p>Verify the installation:</p> <pre><code>bun --version\n</code></pre> <p>Tip</p> <p>Add the <code>PATH</code> export to your <code>.bashrc</code>, <code>.zshrc</code>, or <code>.envrc</code> so <code>bun</code> is available in every new shell session.</p>"},{"location":"getting-started/installation/#environment-variables","title":"Environment Variables","text":"Variable Description Default <code>RLM_MODEL_API_KEY</code> API key for your LLM provider (required) <code>RLM_MODEL_BASE_URL</code> OpenAI-compatible base URL <code>https://openrouter.ai/api/v1</code> <p>You can point fast-rlm at any OpenAI-compatible API:</p> <pre><code>export RLM_MODEL_API_KEY=sk-...\nexport RLM_MODEL_BASE_URL=https://api.openai.com/v1\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<pre><code>python -c \"import fast_rlm; print('fast-rlm installed successfully!')\"\n</code></pre>"},{"location":"getting-started/installation/#dependencies","title":"Dependencies","text":"<p>fast-rlm requires: - Python 3.10+ - <code>openai&gt;=1.0.0</code> - <code>rich&gt;=13.0.0</code> - <code>pyyaml&gt;=6.0</code></p> <p>These are automatically installed with <code>pip install fast-rlm</code>.</p>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":""},{"location":"getting-started/quickstart/#basic-usage","title":"Basic usage","text":"<pre><code>import fast_rlm\n\nresult = fast_rlm.run(\"Generate 50 fruits and count number of r\")\nprint(result[\"results\"])\nprint(result[\"usage\"])\n</code></pre> <p>The returned dict contains:</p> <pre><code>{\n    \"results\": ...,        # the agent's final answer\n    \"log_file\": \"...\",     # path to the JSONL log\n    \"usage\": {\n        \"prompt_tokens\": 12345,\n        \"completion_tokens\": 678,\n        \"total_tokens\": 13023,\n        \"cached_tokens\": 5000,\n        \"reasoning_tokens\": 200,\n        \"cost\": 0.0342\n    }\n}\n</code></pre>"},{"location":"getting-started/quickstart/#arbitrarily-long-context","title":"Arbitrarily long context","text":"<p>The key idea behind RLMs is that the prompt can be arbitrarily long \u2014 far beyond any model's context window. The agent explores it programmatically through the REPL rather than trying to fit it all into a single call.</p> <pre><code>import fast_rlm\n\ntranscripts = open(\"lex_fridman_all_transcripts.txt\").read()  # millions of tokens\n\nresult = fast_rlm.run(\n    \"Here are the transcripts of all Lex Fridman podcasts. \"\n    \"Summarize what the first 5 Machine Learning guests had to say about AGI.\\n\\n\"\n    + transcripts\n)\nprint(result[\"results\"])\n</code></pre> <p>The agent will write code to search, filter, and chunk the transcripts on its own \u2014 no manual splitting required.</p>"},{"location":"getting-started/quickstart/#with-configuration","title":"With configuration","text":"<pre><code>from fast_rlm import run, RLMConfig\n\nconfig = RLMConfig.default()\nconfig.primary_agent = \"minimax/minimax-m2.5\"\nconfig.sub_agent = \"minimax/minimax-m2.5\"\nconfig.max_depth = 5\nconfig.max_money_spent = 2.0\n\nresult = run(\n    \"Count the r's in 50 fruit names\",\n    prefix=\"r_count\",\n    config=config,\n)\n</code></pre>"},{"location":"getting-started/quickstart/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>query</code> <code>str</code> (required) The question or context to process <code>prefix</code> <code>str</code> <code>None</code> Log filename prefix (e.g. <code>\"r_count\"</code> \u2192 <code>r_count_2026-02-23T...</code>) <code>config</code> <code>RLMConfig</code> or <code>dict</code> <code>None</code> Config overrides (see Configuration) <code>verbose</code> <code>bool</code> <code>True</code> Stream engine output to terminal"},{"location":"getting-started/quickstart/#quiet-mode","title":"Quiet mode","text":"<p>To suppress all terminal output and just get the result:</p> <pre><code>result = fast_rlm.run(\"What is 2+2?\", verbose=False)\n</code></pre>"},{"location":"guide/configuration/","title":"Configuration","text":""},{"location":"guide/configuration/#rlmconfig","title":"RLMConfig","text":"<p>All configuration is managed through the <code>RLMConfig</code> dataclass. Every field has a sensible default.</p> <pre><code>from fast_rlm import RLMConfig\n\nconfig = RLMConfig.default()\n</code></pre>"},{"location":"guide/configuration/#fields","title":"Fields","text":"Field Type Default Description <code>primary_agent</code> <code>str</code> <code>z-ai/glm-5</code> Model used for the root agent <code>sub_agent</code> <code>str</code> <code>minimax/minimax-m2.5</code> Model used for child subagents <code>max_depth</code> <code>int</code> <code>3</code> Max recursive subagent depth <code>max_calls_per_subagent</code> <code>int</code> <code>20</code> Max LLM calls a single subagent can make <code>truncate_len</code> <code>int</code> <code>2000</code> Output characters shown to the LLM per step <code>max_money_spent</code> <code>float</code> <code>1.0</code> Hard budget cap in USD \u2014 raises an error if exceeded <code>max_completion_tokens</code> <code>int</code> <code>50000</code> Max total completion tokens across all subagents \u2014 raises an error if exceeded <code>max_prompt_tokens</code> <code>int</code> <code>200000</code> Max total prompt tokens across all subagents \u2014 raises an error if exceeded"},{"location":"guide/configuration/#modifying-config","title":"Modifying config","text":"<p><code>RLMConfig</code> is a dataclass \u2014 just set attributes directly:</p> <pre><code>config = RLMConfig.default()\nconfig.primary_agent = \"openai/gpt-5.2\"\nconfig.max_depth = 5\nconfig.max_money_spent = 3.0\nconfig.max_completion_tokens = 100000\nconfig.max_prompt_tokens = 500000\n</code></pre>"},{"location":"guide/configuration/#using-a-dict","title":"Using a dict","text":"<p>You can also pass a plain dict if you prefer:</p> <pre><code>from fast_rlm import run\n\nresult = run(\n    \"Summarize this document\",\n    config={\"primary_agent\": \"openai/gpt-5.2-codex\", \"max_depth\": 5},\n)\n</code></pre> <p>Dict values are merged on top of the defaults from <code>rlm_config.yaml</code>.</p> <p>Cost tracking depends on your provider</p> <p>Not all API providers return cost information in their responses. OpenRouter includes cost data, but OpenAI and most other providers do not. If your provider doesn't return cost, <code>max_money_spent</code> won't be able to enforce a budget and cost will show as \"Unknown\" in the UI. In that case, use <code>max_completion_tokens</code> and <code>max_prompt_tokens</code> to control spending instead \u2014 these work with any provider since token counts are always returned.</p>"},{"location":"guide/configuration/#how-config-merging-works","title":"How config merging works","text":"<ol> <li>Defaults are loaded from the bundled <code>rlm_config.yaml</code></li> <li>Your overrides (from <code>RLMConfig</code> or <code>dict</code>) are applied on top</li> <li>The merged config is written to a temp file and passed to the engine</li> </ol> <p>This means you only need to specify what you want to change \u2014 everything else keeps its default value.</p>"},{"location":"guide/configuration/#model-names","title":"Model names","text":"<p>fast-rlm uses OpenRouter by default, so model names follow the OpenRouter format: <code>provider/model-name</code>.</p> <p>Since agents write and execute Python code in a REPL, choose models that are strong at coding. Recommended models:</p> Model Name GPT-5.2 Codex <code>openai/gpt-5.2-codex</code> Claude Sonnet 4.6 <code>anthropic/claude-sonnet-4-6</code> Gemini 3.1 Pro Preview <code>google/gemini-3.1-pro-preview</code> MiniMax M2.5 <code>minimax/minimax-m2.5</code> GLM-5 <code>z-ai/glm-5</code> <p>Browse the full list at openrouter.ai/models.</p> <p>Using a different provider</p> <p>If you're pointing <code>RLM_MODEL_BASE_URL</code> at a different API (e.g. OpenAI directly), use that provider's model names instead (e.g. <code>gpt-5.2-codex</code> instead of <code>openai/gpt-5.2-codex</code>).</p>"},{"location":"guide/log-viewer/","title":"Log Viewer","text":"<p>Every run saves a <code>.jsonl</code> log file to <code>logs/</code>. The log captures every step: code generated, output produced, usage stats, and final results across all agents and sub-agents.</p> <p></p>"},{"location":"guide/log-viewer/#stats-default","title":"Stats (default)","text":"<p>Print a summary of any log file \u2014 no extra dependencies required:</p> <pre><code>fast-rlm-log logs/run_2026-02-23T20-33-30-936Z.jsonl\n</code></pre> <p>Output:</p> <pre><code>Log entries:  42\nTotal runs:   5\nRoot runs:    1\nMax depth:    2\nTotal tokens: 45,230\nTotal cost:   $0.034521\n</code></pre>"},{"location":"guide/log-viewer/#interactive-tui","title":"Interactive TUI","text":"<p>For a full interactive viewer with code highlighting, step navigation, and reasoning traces:</p> <pre><code>fast-rlm-log logs/run_2026-02-23T20-33-30-936Z.jsonl --tui\n</code></pre> <p>Requires Bun</p> <p>The TUI viewer is built with OpenTUI and requires Bun. Dependencies are installed automatically on first run.</p>"},{"location":"guide/log-viewer/#tui-controls","title":"TUI controls","text":"Key Action <code>Up</code> / <code>Down</code> Navigate steps in current run <code>Left</code> / <code>Right</code> Go to parent / child subagent <code>Tab</code> / <code>Shift+Tab</code> Next / previous sibling subagent <code>H</code> / <code>J</code> Scroll code panel up / down <code>K</code> / <code>L</code> Scroll output panel up / down <code>R</code> Toggle reasoning trace modal <code>O</code> Toggle final output modal <code>q</code> / <code>Ctrl+C</code> Quit"},{"location":"guide/log-viewer/#programmatic-access","title":"Programmatic access","text":"<p>The log file path is included in every <code>run()</code> result:</p> <pre><code>import fast_rlm\n\nresult = fast_rlm.run(\"What is 2+2?\")\nprint(result[\"log_file\"])  # e.g. \"./logs/run_2026-02-23T20-33-30-936Z.jsonl\"\n</code></pre> <p>The log is standard JSONL \u2014 each line is a JSON object you can parse with any tool.</p>"},{"location":"guide/tips/","title":"Best Practices &amp; Troubleshooting","text":""},{"location":"guide/tips/#best-practices","title":"Best practices","text":""},{"location":"guide/tips/#place-your-task-at-the-start-or-end-of-the-prompt","title":"Place your task at the start or end of the prompt","text":"<p>RLMs explore long prompts by writing code, slicing context, and finding keywords \u2014 the REPL restricts the amount of context the LLM can see (you can configure this using the config param <code>truncate_len</code>). If your task description is buried in the middle, the agent may struggle to find it. Always put the task at the very top or bottom of the prompt so the agent sees it immediately.</p> <pre><code># Good \u2014 task is at the top, data follows\nresult = fast_rlm.run(\n    \"Summarize the key takeaways from these meeting notes.\\n\\n\"\n    + meeting_notes\n)\n\n# Also good \u2014 data first, task at the end\nresult = fast_rlm.run(\n    meeting_notes + \"\\n\\n\"\n    \"Given the meeting notes above, summarize the key takeaways.\"\n)\n</code></pre>"},{"location":"guide/tips/#mark-structured-data-with-backtick-blocks","title":"Mark structured data with backtick blocks","text":"<p>If your data has a specific structure (JSON, CSV, XML, etc.), wrap it in fenced code blocks and tell the agent what format it is. This makes it much easier for the agent to extract and parse the data programmatically.</p> <pre><code>result = fast_rlm.run(\n    \"The following is a CSV of sales records. \"\n    \"Find the top 5 products by revenue.\\n\\n\"\n    \"```csv\\n\"\n    + csv_data +\n    \"\\n```\"\n)\n</code></pre>"},{"location":"guide/tips/#use-strong-coding-models","title":"Use strong coding models","text":"<p>RLM agents write and execute Python code to solve tasks \u2014 model quality matters a lot. Check coding benchmarks and pick models that perform well on code generation. See the Configuration page for recommended models.</p>"},{"location":"guide/tips/#add-domain-context-when-needed","title":"Add domain context when needed","text":"<p>If the domain is obscure or specialized, you can feed additional documentation into the prompt. Just make sure you tell the agent how the data is organized so it can navigate efficiently.</p> <pre><code>docs = open(\"internal_api_reference.md\").read()\nlogs = open(\"error_logs.txt\").read()\n\nresult = fast_rlm.run(\n    \"I have provided two documents below, separated by markdown ## headers.\\n\"\n    \"## API Reference\\n\" + docs + \"\\n\\n\"\n    \"## Error Logs\\n\" + logs + \"\\n\\n\"\n    \"Using the API reference, diagnose why the errors in the logs are occurring.\"\n)\n</code></pre>"},{"location":"guide/tips/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guide/tips/#check-your-logs","title":"Check your logs","text":"<p>Every run produces a <code>.jsonl</code> log file. Use the Log Viewer to inspect what the agent is doing step by step \u2014 what code it wrote, what output it saw, and where it went wrong.</p> <pre><code>fast-rlm-log logs/run_xxx.jsonl --tui\n</code></pre>"},{"location":"guide/tips/#start-with-strict-limits","title":"Start with strict limits","text":"<p>In early experiments, keep <code>max_depth</code>, <code>max_calls_per_subagent</code>, and <code>max_money_spent</code> low. This lets you verify the agent is on the right track before scaling up. If the agent consistently fails or goes off course, it usually means the task prompt needs adjustment rather than more budget.</p> <pre><code>from fast_rlm import run, RLMConfig\n\nconfig = RLMConfig.default()\nconfig.max_depth = 2\nconfig.max_calls_per_subagent = 10\nconfig.max_money_spent = 0.50\n\nresult = run(\"Your task here...\", config=config)\n</code></pre>"},{"location":"guide/tips/#things-to-do-before-increasing-budget","title":"Things to do before increasing budget","text":"<p>If results are poor, work through this list before raising <code>max_depth</code>, <code>max_calls_per_subagent</code>, or <code>max_money_spent</code>:</p> <ol> <li>Review the logs \u2014 Use the Log Viewer to see exactly where the agent went wrong. This tells you what to fix.</li> <li>Make the task description clearer \u2014 Be specific about what you want. Vague prompts lead to wasted calls.</li> <li>Move the task to the top or bottom of the prompt \u2014 Don't let it get lost in the middle of a large context.</li> <li>Try a stronger coding model \u2014 Switch <code>primary_agent</code> or <code>sub_agent</code> to a model with better coding benchmarks. A smarter model often solves in fewer calls what a weaker model can't solve at all.</li> <li>Mark data formats explicitly \u2014 Wrap JSON, CSV, or structured data in fenced code blocks and name the format in the prompt.</li> <li>Inject additional reference docs \u2014 If the domain is specialized, add relevant documentation to the context so the agent doesn't have to guess.</li> <li>Increase <code>truncate_len</code> \u2014 If the agent is missing important output because it gets clipped, bump this value so it can see more per step.</li> </ol>"}]}